{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Algo Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from src import agent, train, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = utils.ReplayMemory(10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Pong-v4')\n",
    "\n",
    "s_t = env.reset()\n",
    "s_t1 = None\n",
    "\n",
    "for i in range(20):\n",
    "    a = random.randrange(env.action_space.n)\n",
    "    s_t1, r_t, done, _ = env.step(a)\n",
    "    rm.store(s_t, a, r_t, done)\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        s_t = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_sample = rm.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 84, 84])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_sample[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_mem_size = int(5e5)\n",
    "batch_size = 64\n",
    "num_episodes = int(1e3)\n",
    "num_steps = int(1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(replay_mem_size, batch_size):\n",
    "    agt = agent.DQNAgent()\n",
    "    replay_mem = utils.ReplayMemory(replay_mem_size, batch_size)\n",
    "    obs_history = utils.ObsHistory()\n",
    "    optimizer = optim.RMSprop(agt.qnet.parameters())\n",
    "    env = gym.envs.make('Pong-v4')\n",
    "    \n",
    "    return agt, replay_mem, obs_history, optimizer, env\n",
    "\n",
    "def act_step(obs_history, agt, env):\n",
    "    phi_t = obs_history.get_phi()\n",
    "    a_t = agt.act(phi_t)\n",
    "    s_t1, r_t, done, _ = env.step(a_t)\n",
    "    \n",
    "    return a_t, s_t1, r_t, done\n",
    "\n",
    "def store_step(s_t, a_t, r_t, done, s_t1, obs_history, replay_mem):\n",
    "    obs_history.store(s_t1)\n",
    "    replay_memory.store(s_t, a_t, r_t, done)\n",
    "\n",
    "def gradient_step(replay_memory, agt):\n",
    "    if replay_memory.size() > replay_memory.sample_size + 3:\n",
    "        mini_batch = replay_memory.sample()\n",
    "\n",
    "        agt.optimizer.zero_grad()\n",
    "        loss = train.mini_batch_loss(mini_batch, agt)\n",
    "        loss.backward()\n",
    "        agt.optimizer.step()\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "def save_params(agt, optimizer, save_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': agt.qnet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, save_path)\n",
    "\n",
    "def load_params(agt, load_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    agt.qnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agt.opimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def reset_episode(env, obs_history):\n",
    "    s_t = env.reset()\n",
    "    obs_history.reset(s_t)\n",
    "    done = False\n",
    "    \n",
    "    return s_t, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agt, replay_mem, obs_history, env = initialize(replay_mem_size, batch_size)\n",
    "\n",
    "for episode in range(num_episodes):  # loop over episodes\n",
    "    s_t, done = reset_episode(env, obs_history)\n",
    "    \n",
    "    while not done:  # loop over steps in episode\n",
    "        a_t, s_t1, r_t, done = act_step(obs_history, agt, env)\n",
    "        store_step(s_t, a_t, r_t, done, s_t1, obs_history, replay_mem)\n",
    "        \n",
    "        s_t = s_t1\n",
    "        \n",
    "        gradient_step(replay_memory, agt)\n",
    "            \n",
    "    if episode % 10 == 9:\n",
    "        save_params(agt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Go Through One Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "from src import agent, dqn, train, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs = env.reset()\n",
    "\n",
    "for i in range(20000):\n",
    "    a = random.randrange(env.action_space.n)\n",
    "    _, _, done, _ = env.step(a)\n",
    "    time.sleep(.01)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs = env.reset()\n",
    "init_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s1, r, done, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize DQN objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_mem_size = int(1e6)\n",
    "mini_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt = agent.DQNAgent()\n",
    "replay_memory = utils.ReplayMemory(replay_mem_size, mini_batch_size)\n",
    "obs_history = utils.ObsHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin new episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_init = env.reset()  # reset environment to start new episode\n",
    "obs_history.reset(obs_init)  # reset observations for new episode\n",
    "done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = obs_history.phi\n",
    "a = agt.act(phi)\n",
    "obs, rew, done, _ = env.step(a)\n",
    "obs_history.store(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_1 = obs_history.phi\n",
    "replay_memory.store((phi, a, rew, phi_1, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Step\n",
    "\n",
    "[x] Dummy transitions function for testing.\n",
    "\n",
    "[x] Make `r` reward vector from transitions.\n",
    "\n",
    "[x] Make $\\max_{a}Q(s', a')$ vector from transitions.\n",
    "\n",
    "[x] Make `y` target vector from transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "                        ['phi', 'a', 'r', 'phi_1', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_transitions(n):\n",
    "    transitions = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        phi = torch.empty(4, 84, 84).random_(0, 255)\n",
    "        phi_1 = torch.empty(4, 84, 84).random_(0, 255)\n",
    "        a = np.random.randint(0, 6)\n",
    "        r = np.random.randint(0, 2)\n",
    "        done = False if np.random.randint(0, 2) == 0 else True\n",
    "        \n",
    "        transitions.append(Transition(phi, a, r, phi_1, done))\n",
    "    \n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = dummy_transitions(3)\n",
    "    \n",
    "phi, a, r, phi_1, done = zip(*transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y(transitions, agt):\n",
    "    y = []\n",
    "    \n",
    "    for tr in transitions:\n",
    "        if tr.done:\n",
    "            y.append(tr.r)\n",
    "        else:\n",
    "            x = tr.phi.unsqueeze(0)\n",
    "            y.append(tr.r + .99 * agt.get_best_values(x).item())\n",
    "            \n",
    "    return torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = make_y(transitions, agt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_vals(transitions, agt):\n",
    "    phis = []\n",
    "\n",
    "    for tr in transitions:\n",
    "        phis.append(tr.phi)\n",
    "\n",
    "    x = torch.stack(phis)\n",
    "    return agt.get_best_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmax = get_max_vals(transitions, agt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y, qmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_loss(transitions, agt):\n",
    "    y = make_y(transitions, agt)\n",
    "    qmax = get_max_vals(transitions, agt)\n",
    "    \n",
    "    loss = nn.MSELoss(reduction='mean')\n",
    "    return loss(y, qmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss = mini_batch_loss(transitions, agt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.RMSprop(agt.qnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss = mini_batch_loss(transitions, agt)\n",
    "mb_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random agent on Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obss = []\n",
    "obss.append(env.reset())\n",
    "\n",
    "for _ in range(1000):\n",
    "    a = np.random.choice(env.action_space.n)\n",
    "    obs, rew, done, _ = env.step(a)\n",
    "    obss.append(obs)\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
