{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] Implement action repetition every 4 frames.\n",
    "- [x] Change model architecture\n",
    "- [x] Huber loss in gradient step.\n",
    "- [x] Save model and optimizer.\n",
    "- [x] Time training.\n",
    "- [x] Use tricks from paper (data collection, rmsprop)\n",
    "- [x] Optimize batch loss calc\n",
    "- [x] Save statistics from paper.\n",
    "- [x] Pick out frames with obvious Q vals and graph them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    \"\"\"\n",
    "    Turn game frame into small, square, grayscale image.\n",
    "    \"\"\"\n",
    "    pipeline = transforms.Compose([\n",
    "        transforms.ToPILImage(),  # turn numpy ndarray into PIL image\n",
    "        transforms.Grayscale(),  # convert image to grayscale\n",
    "        transforms.Resize((84,84)),  # resize image to 84 x 84\n",
    "        transforms.ToTensor()  # convert PIL image to torch tensor\n",
    "    ])\n",
    "\n",
    "    return pipeline(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32,\n",
    "                               kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64,\n",
    "                               kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64,\n",
    "                               kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view((x.shape[0], -1))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Create an agent that uses DQN to guide its policy.\n",
    "\n",
    "        This agent contains:\n",
    "            - A history of the states it has been in, for its current episode.\n",
    "            - A history of processed states for its current episode.\n",
    "            - The recent transitions it has made.\n",
    "            - The epsilon greedy strategy it's using.\n",
    "         \"\"\"\n",
    "        self.env = gym.envs.make('PongNoFrameskip-v4')\n",
    "\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.qnet = QNet(self.env.action_space.n).to(device=self.device)\n",
    "        self.target = QNet(self.env.action_space.n).to(device=self.device)\n",
    "        self.target.load_state_dict(self.qnet.state_dict())\n",
    "        self.target.eval()\n",
    "        self.optimizer = optim.Adam(self.qnet.parameters())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.annealing_steps = int(3e5)\n",
    "        self.min_epsilon = .1\n",
    "        self.step_size = (self.epsilon - self.min_epsilon) / self.annealing_steps\n",
    "\n",
    "    def act(self, phi):\n",
    "        # select action using epsilon greedy strategy\n",
    "        u = random.random()\n",
    "        if u < self.epsilon:  # with probability epsilon, select action uniformly at random\n",
    "            a = random.randrange(self.env.action_space.n)\n",
    "        else:  # otherwise, select best action\n",
    "            phi = phi.unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                a = self.qnet(phi).argmax(1)\n",
    "\n",
    "        self._update_epsilon()\n",
    "        return a\n",
    "    \n",
    "    def _update_epsilon(self):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon -= self.step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, n, sample_size):\n",
    "        self.n = n\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.xs = torch.empty((self.n, 84, 84), dtype=torch.float)\n",
    "        self.actions = torch.empty(self.n, dtype=torch.long)\n",
    "        self.rewards = torch.empty(self.n, dtype=torch.float)\n",
    "        self.dones = torch.empty(self.n, dtype=torch.uint8)\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "\n",
    "\n",
    "    def sample(self, k=None):\n",
    "        if k is None:\n",
    "            k = self.sample_size\n",
    "\n",
    "        # start at 3 because we need 4 frames\n",
    "        # end at mem_size-1 because we need phi_t1\n",
    "        idxs = torch.randint(3, self.size-1, (k,))\n",
    "\n",
    "        phi, phi_1 = self.get_phis(idxs)\n",
    "        return (phi,\n",
    "                self.actions.index_select(0, idxs),\n",
    "                self.rewards.index_select(0, idxs),\n",
    "                phi_1,\n",
    "                self.dones.index_select(0,idxs))\n",
    "\n",
    "    def store(self, s, a, r, done):\n",
    "        x = process_frame(s)\n",
    "\n",
    "        self.xs[self.idx] = x\n",
    "        self.actions[self.idx] = a\n",
    "        self.rewards[self.idx] = r\n",
    "        self.dones[self.idx] = int(done)\n",
    "\n",
    "        self.idx  = (self.idx + 1) % self.n\n",
    "        self.size = min(self.size + 1, self.n)\n",
    "\n",
    "    def get_phi(self, i):\n",
    "        return self.xs[i-3:i+1]\n",
    "\n",
    "    def get_phis(self, idxs):\n",
    "        phi_t = []\n",
    "        phi_t1 = []\n",
    "\n",
    "        for i in idxs:\n",
    "            phi_t.append(self.get_phi(i))\n",
    "            phi_t1.append(self.get_phi(i+1))\n",
    "\n",
    "        return torch.stack(phi_t), torch.stack(phi_t1)\n",
    "\n",
    "\n",
    "    def get_transition(self, t):\n",
    "        phi_t = self.get_phi(t)\n",
    "        phi_t1 = self.get_phi(t + 1)\n",
    "\n",
    "\n",
    "        return (phi_t, self.actions[t], self.rewards[t], phi_t1, self.dones[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsHistory:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.obs4 = None\n",
    "        self.phi = None\n",
    "\n",
    "    def reset(self, obs_init):\n",
    "        obs_init_p = process_frame(obs_init)\n",
    "        self.obs4 = deque(4 * [obs_init_p])\n",
    "        self.phi = torch.cat(tuple(self.obs4))\n",
    "\n",
    "    def store(self, obs):\n",
    "        obs = process_frame(obs)\n",
    "        self.obs4.append(obs)\n",
    "        self.obs4.popleft()\n",
    "\n",
    "        self.phi = torch.cat(tuple(self.obs4))\n",
    "\n",
    "    def get_phi(self):\n",
    "        return self.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(replay_mem_size, batch_size):\n",
    "    agt = DQNAgent()\n",
    "    replay_mem = ReplayMemory(replay_mem_size, batch_size)\n",
    "    obs_history = ObsHistory()\n",
    "    env = gym.envs.make('PongNoFrameskip-v4')\n",
    "    train_stats = TrainingStats(agt, 'dqn_vanilla.csv')\n",
    "    \n",
    "    return agt, replay_mem, obs_history, env, train_stats\n",
    "\n",
    "\n",
    "def mini_batch_to_tensor(mini_batch, agt):\n",
    "    phi, a, r, phi_1, dones = mini_batch\n",
    "    \n",
    "    phi = phi.to(agt.device, non_blocking=True)\n",
    "    phi_1 = phi.to(agt.device, non_blocking=True).detach()\n",
    "    a = a.to(agt.device, torch.long, non_blocking=True)\n",
    "    r = r.to(agt.device, torch.float, non_blocking=True)\n",
    "    dones = dones.to(agt.device, torch.float, non_blocking=True)\n",
    "    \n",
    "    return phi, a, r, phi_1, dones\n",
    "    \n",
    "    \n",
    "def mini_batch_loss(mini_batch, gamma, agt):\n",
    "    phi, a, r, phi_1, dones = mini_batch_to_tensor(mini_batch, agt)\n",
    "    \n",
    "    q_phi_1 = agt.target(phi_1).max(1)[0] * (1 - dones)\n",
    "    y = (r + gamma * q_phi_1).detach()\n",
    "    q_trans = agt.qnet(phi).gather(1, a.unsqueeze(1))\n",
    "\n",
    "    loss = nn.SmoothL1Loss()\n",
    "    return loss(y, q_trans)\n",
    "\n",
    "\n",
    "def gradient_step(replay_mem, agt, gamma):\n",
    "    if replay_mem.size > replay_mem.sample_size + 3:\n",
    "        mini_batch = replay_mem.sample()\n",
    "        \n",
    "        agt.optimizer.zero_grad()\n",
    "        loss = mini_batch_loss(mini_batch, gamma, agt)\n",
    "        loss.backward()\n",
    "        agt.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "def save_params(agt, episodes, total_steps, save_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': agt.qnet.state_dict(),\n",
    "        'optimizer_state_dict': agt.optimizer.state_dict(),\n",
    "        'episodes': episodes,\n",
    "        'total_steps': total_steps\n",
    "    }, save_path)\n",
    "\n",
    "    \n",
    "def load_params(load_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    agt.qnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agt.opimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return chekpoint['episodes'], checkpoint['total_steps']\n",
    "\n",
    "\n",
    "def reset_episode(env, obs_history):\n",
    "    s_t = env.reset()\n",
    "    obs_history.reset(s_t)\n",
    "    done = False\n",
    "    \n",
    "    return s_t, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_transitions(k, n):\n",
    "    env = gym.envs.make('PongNoFrameskip-v4')\n",
    "    s_t = env.reset()\n",
    "    \n",
    "    replay_mem = ReplayMemory(n, k)\n",
    "    \n",
    "    for i in range(n):\n",
    "        a_t = random.randrange(env.action_space.n)\n",
    "        s_t1, r_t, done, _ = env.step(a_t)\n",
    "        replay_mem.store(s_t, a_t, r_t, done)\n",
    "        \n",
    "        s_t = s_t1\n",
    "        \n",
    "        if done:\n",
    "            s_t = env.reset()\n",
    "            \n",
    "    return replay_mem.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_phi(frames):\n",
    "    frames = [process_frame(s) for s in frames]\n",
    "    phi = torch.cat(frames)\n",
    "\n",
    "    return phi\n",
    "\n",
    "def get_rand_phis(k, n):\n",
    "    frames = []\n",
    "    env = gym.envs.make('PongNoFrameskip-v4')\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(n):\n",
    "        a = random.randrange(env.action_space.n)\n",
    "        s_t1, r_t, done, _ = env.step(a)\n",
    "        frames.append(s_t1)\n",
    "\n",
    "        if done:\n",
    "            s_t = env.reset()\n",
    "\n",
    "    idxs = random.sample(range(3, n), k)\n",
    "    phis = [frames_to_phi(frames[i-3:i+1]) for i in idxs]\n",
    "\n",
    "    return phis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingStats:\n",
    "    \n",
    "    def __init__(self, agt, result_file):\n",
    "        self.agt = agt\n",
    "        self.benchmark_frames = torch.stack(get_rand_phis(10, 10000)).to(agt.device)\n",
    "        self.result_file = result_file\n",
    "        \n",
    "        with open(self.result_file, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['ep_rew', 'ep_steps', 'ep_num', 'total_steps', 'ep_dur', 'avg_qvals'])\n",
    "        \n",
    "    def store(self, ep_reward, ep_steps, ep_num, total_steps, ep_dur):\n",
    "        avg_qvals = self.get_frames_avg_qval()\n",
    "        \n",
    "        with open(self.result_file, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([ep_reward, ep_steps, ep_num, total_steps, ep_dur, avg_qvals])\n",
    "            \n",
    "        \n",
    "        if ep_num % 300 == 0:\n",
    "            save_params(self.agt, ep_num, total_steps,\n",
    "                        'dqn_agt_{}.pt'.format(ep_num))\n",
    "            \n",
    "        \n",
    "        print('Episode {}:'.format(ep_num))\n",
    "        print('Reward: {}'.format(ep_reward))\n",
    "        print('Total steps: {}'.format(total_steps))\n",
    "        print('Avg qvals: {:.5f}'.format(avg_qvals))\n",
    "        print('Duration: {:.2f}'.format(ep_dur))\n",
    "        print('===========================================')\n",
    "        \n",
    "\n",
    "    def get_frames_avg_qval(self):\n",
    "        qvals = self.agt.target(self.benchmark_frames).max(1)[0]\n",
    "\n",
    "        return torch.mean(qvals).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .99\n",
    "replay_mem_size = int(4e5)\n",
    "batch_size = 64\n",
    "num_episodes = int(5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agt, replay_mem, obs_history, env, train_stats = \\\n",
    "    initialize(replay_mem_size, batch_size)\n",
    "rand_phis = get_rand_phis(10, 10000)\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    t0 = time.time()\n",
    "    s_t, done = reset_episode(env, obs_history)\n",
    "    a_t = 0\n",
    "    \n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        if ep_steps % 4 == 0: # select action every 4 frames\n",
    "            phi_t = obs_history.get_phi()\n",
    "            a_t = agt.act(phi_t)\n",
    "        s_t1, r_t, done, _ = env.step(a_t)\n",
    "        \n",
    "        obs_history.store(s_t1)\n",
    "        replay_mem.store(s_t, a_t, r_t, done)\n",
    "        s_t = s_t1\n",
    "        \n",
    "        if total_steps > 50000:\n",
    "            loss_val = gradient_step(replay_mem, agt, gamma)\n",
    "        \n",
    "        if total_steps % 5000 == 0:\n",
    "            agt.target.load_state_dict(agt.qnet.state_dict())\n",
    "            \n",
    "\n",
    "        ep_reward += r_t\n",
    "        ep_steps += 1\n",
    "        total_steps += 1\n",
    "        \n",
    "    t1 = time.time()\n",
    "    train_stats.store(ep_reward, ep_steps, episode, \n",
    "                      total_steps, t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('PongNoFrameskip-v4')\n",
    "agt = QNet(env.action_space.n)\n",
    "checkpoint = torch.load('../adam_player/dqn_agt_1000.pt', map_location='cpu')\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "state_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "s_t = env.reset()\n",
    "oh = ObsHistory()\n",
    "oh.reset(s_t)\n",
    "\n",
    "while not done:\n",
    "    phi_t = oh.get_phi().unsqueeze(0)\n",
    "    env.render()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        states.append(s_t)\n",
    "        state_vals.append(agt(phi_t))\n",
    "        print(agt(phi_t))\n",
    "        if random.random() < .1:\n",
    "            a_t = random.randrange(env.action_space.n)\n",
    "        else:\n",
    "            a_t = agt(phi_t).argmax(1)\n",
    "        print(a_t)\n",
    "    \n",
    "    s_t, r, done, _ = env.step(a_t)\n",
    "    \n",
    "    oh.store(s_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "575, 505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(states[505]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_loss_val = state_vals[505]\n",
    "before_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: \"NOOP\",\n",
    "    1: \"FIRE\",\n",
    "    2: \"UP\",\n",
    "    3: \"RIGHT\",\n",
    "    4: \"LEFT\",\n",
    "5: \"DOWN\",\n",
    "['No-Op', 'Fire', 'Up', 'Right', 'Left', 'Down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_loss_val.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(np.arange(6), after_start_val.numpy()[0])\n",
    "plt.xticks(np.arange(6), ['No-Op', 'Fire', 'Up', 'Right', 'Left', 'Down'])\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Q Value')\n",
    "plt.title('Action Values After Start')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_start_val = state_vals[575]\n",
    "after_start_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_data = pd.read_csv('../pong-ppo/progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_data = pd.read_csv('../pong-dqn/progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_ep_rew = ppo_data.eprewmean.dropna()\n",
    "dqn_ep_rew = dqn_data.eprewmean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(ppo_ep_rew, label='PPO')\n",
    "ax.plot(dqn_ep_rew, label='DQN')\n",
    "\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.title('PPO vs DQN Pong Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
