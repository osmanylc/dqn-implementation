{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# from src import agent, dqn, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparams to paper's numbers\n",
    "N = int(1e7)\n",
    "epsilon_generator = (1 - (i * .9/1e6) for i in range(int(1e6)))\n",
    "# mini-batch size = 32\n",
    "# optimizer RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Algo Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_mem_size = int(1e6)\n",
    "mini_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt = agent.DQNAgent()\n",
    "replay_memory = utils.ReplayMemory(replay_mem_size, mini_batch_size)\n",
    "obs_history = utils.ObsHistory()\n",
    "\n",
    "for episode in range(M):  # loop over episodes\n",
    "    obs_init = env.reset()  # reset environment to start new episode\n",
    "    obs_history.reset(obs_init)  # reset observations for new episode\n",
    "    done = False\n",
    "    \n",
    "    while not done:  # loop over steps in episode\n",
    "        phi = obs_history.get_phi()\n",
    "        a = agt.act(phi)\n",
    "        obs, rew, done, _ = env.step(a)\n",
    "        obs_history.store(obs)\n",
    "        \n",
    "        # store transition\n",
    "        phi_1 = obs_history.phi\n",
    "        replay_memory.store((phi, a, rew, phi_1))\n",
    "        \n",
    "        # sample minibatch of transitions\n",
    "        mini_batch = replay_memory.sample()\n",
    "        \n",
    "        # perform a mini-batch of stochastic gradient descent\n",
    "        grad = 0\n",
    "        for transition in mini_batch:\n",
    "            phi_j, a_j, r_j, phi_j1 = transition\n",
    "            \n",
    "            # calculate TD error for each transition\n",
    "            if phi_jp1 is terminal:\n",
    "                y_j = r_j\n",
    "            else:\n",
    "                y_j = r_j + lam * argmax(Q(phi_jp1))\n",
    "            # calculate the gradient from transition\n",
    "            grad += get_grad(y_j, Q, phi_j, a_j)\n",
    "            \n",
    "        grad = 1/K * grad\n",
    "        Q.gradient_descent(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s_seq = []\n",
    "        self.phi_seq = []\n",
    "        self.replay_mem = ReplayMemory()\n",
    "\n",
    "        self.q_net = QNet()\n",
    "        self.epsilon = .9\n",
    "        \n",
    "        self.env = gym.envs.make('PongNoFrameskip-v4')\n",
    "    \n",
    "    def act(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def _train(self):\n",
    "        pass\n",
    "    \n",
    "    def _update_epsilon(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, N, sample_size):\n",
    "        self.N = N\n",
    "        self.sample_size = sample_size\n",
    "        self.transitions = deque()\n",
    "        \n",
    "    def sample(self, k=None):\n",
    "        if k is None:\n",
    "            k = self.sample_size\n",
    "        return np.random.choice(self.transitions, k)\n",
    "\n",
    "    def add(e):\n",
    "        if len(self.transitions >= self.N):\n",
    "            self.transitions.popleft()\n",
    "        self.transitions.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random agent on Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.envs.make('PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obss = []\n",
    "obss.append(env.reset())\n",
    "\n",
    "for _ in range(1000):\n",
    "    a = np.random.choice(env.action_space.n)\n",
    "    obs, rew, done, _ = env.step(a)\n",
    "    obss.append(obs)\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
