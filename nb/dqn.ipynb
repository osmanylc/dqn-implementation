{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Algo Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from src import agent, train, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = utils.ReplayMemory(10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Pong-v4')\n",
    "\n",
    "s_t = env.reset()\n",
    "s_t1 = None\n",
    "\n",
    "for i in range(20):\n",
    "    a = random.randrange(env.action_space.n)\n",
    "    s_t1, r_t, done, _ = env.step(a)\n",
    "    rm.store(s_t, a, r_t, done)\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        s_t = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_sample = rm.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_sample[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_mem_size = int(5e5)\n",
    "mini_batch_size = 64\n",
    "num_episodes = int(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #0. Avg loss: 0.015443761451479193. Episode reward: -21.0\n",
      "Episode #1. Avg loss: 0.018243928530585273. Episode reward: -20.0\n"
     ]
    }
   ],
   "source": [
    "agt = agent.DQNAgent()\n",
    "replay_memory = utils.ReplayMemory(replay_mem_size, mini_batch_size)\n",
    "obs_history = utils.ObsHistory()\n",
    "optimizer = optim.RMSprop(agt.qnet.parameters())\n",
    "\n",
    "env = gym.envs.make('Pong-v4')\n",
    "\n",
    "for episode in range(num_episodes):  # loop over episodes\n",
    "    s_t = env.reset()  # reset environment to start new episode\n",
    "    obs_history.reset(s_t)  # reset observations for new episode\n",
    "    done = False\n",
    "        \n",
    "    cumulative_loss = 0\n",
    "    n_steps = 0\n",
    "    ep_r = 0\n",
    "    while not done:  # loop over steps in episode\n",
    "        phi_t = obs_history.get_phi()\n",
    "        a_t = agt.act(phi_t)\n",
    "        s_t1, r_t, done, _ = env.step(a_t)\n",
    "        obs_history.store(s_t1)\n",
    "        n_steps += 1\n",
    "        ep_r += r_t\n",
    "        \n",
    "        # store transition\n",
    "        phi_t1 = obs_history.get_phi()\n",
    "        replay_memory.store(s_t, a_t, r_t, done)\n",
    "        s_t = s_t1\n",
    "        \n",
    "        # perform a mini-batch of stochastic gradient descent\n",
    "        if replay_memory.size() > replay_memory.sample_size + 3:\n",
    "            mini_batch = replay_memory.sample()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = train.mini_batch_loss(mini_batch, agt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss.item()\n",
    "            \n",
    "\n",
    "    print('Episode #{}. Avg loss: {}. Episode reward: {}'\n",
    "          .format(episode, cumulative_loss / n_steps, ep_r))\n",
    "    if episode % 10 == 9:\n",
    "        torch.save(agt.qnet.state_dict(), 'dqn_agt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Go Through One Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "from src import agent, dqn, train, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs = env.reset()\n",
    "\n",
    "for i in range(20000):\n",
    "    a = random.randrange(env.action_space.n)\n",
    "    _, _, done, _ = env.step(a)\n",
    "    time.sleep(.01)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs = env.reset()\n",
    "init_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s1, r, done, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize DQN objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_mem_size = int(1e6)\n",
    "mini_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt = agent.DQNAgent()\n",
    "replay_memory = utils.ReplayMemory(replay_mem_size, mini_batch_size)\n",
    "obs_history = utils.ObsHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin new episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_init = env.reset()  # reset environment to start new episode\n",
    "obs_history.reset(obs_init)  # reset observations for new episode\n",
    "done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = obs_history.phi\n",
    "a = agt.act(phi)\n",
    "obs, rew, done, _ = env.step(a)\n",
    "obs_history.store(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_1 = obs_history.phi\n",
    "replay_memory.store((phi, a, rew, phi_1, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Step\n",
    "\n",
    "[x] Dummy transitions function for testing.\n",
    "\n",
    "[x] Make `r` reward vector from transitions.\n",
    "\n",
    "[x] Make $\\max_{a}Q(s', a')$ vector from transitions.\n",
    "\n",
    "[x] Make `y` target vector from transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "                        ['phi', 'a', 'r', 'phi_1', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_transitions(n):\n",
    "    transitions = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        phi = torch.empty(4, 84, 84).random_(0, 255)\n",
    "        phi_1 = torch.empty(4, 84, 84).random_(0, 255)\n",
    "        a = np.random.randint(0, 6)\n",
    "        r = np.random.randint(0, 2)\n",
    "        done = False if np.random.randint(0, 2) == 0 else True\n",
    "        \n",
    "        transitions.append(Transition(phi, a, r, phi_1, done))\n",
    "    \n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = dummy_transitions(3)\n",
    "    \n",
    "phi, a, r, phi_1, done = zip(*transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y(transitions, agt):\n",
    "    y = []\n",
    "    \n",
    "    for tr in transitions:\n",
    "        if tr.done:\n",
    "            y.append(tr.r)\n",
    "        else:\n",
    "            x = tr.phi.unsqueeze(0)\n",
    "            y.append(tr.r + .99 * agt.get_best_values(x).item())\n",
    "            \n",
    "    return torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = make_y(transitions, agt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_vals(transitions, agt):\n",
    "    phis = []\n",
    "\n",
    "    for tr in transitions:\n",
    "        phis.append(tr.phi)\n",
    "\n",
    "    x = torch.stack(phis)\n",
    "    return agt.get_best_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmax = get_max_vals(transitions, agt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y, qmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_loss(transitions, agt):\n",
    "    y = make_y(transitions, agt)\n",
    "    qmax = get_max_vals(transitions, agt)\n",
    "    \n",
    "    loss = nn.MSELoss(reduction='mean')\n",
    "    return loss(y, qmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss = mini_batch_loss(transitions, agt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.RMSprop(agt.qnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss = mini_batch_loss(transitions, agt)\n",
    "mb_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random agent on Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obss = []\n",
    "obss.append(env.reset())\n",
    "\n",
    "for _ in range(1000):\n",
    "    a = np.random.choice(env.action_space.n)\n",
    "    obs, rew, done, _ = env.step(a)\n",
    "    obss.append(obs)\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
