{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Algo Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from src import agent, train, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect 10 random frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_mem_size = int(1e6)\n",
    "batch_size = 32\n",
    "num_episodes = int(1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_phi(frames):\n",
    "    frames = [utils.process_frame(s) \n",
    "              for s in frames]\n",
    "    phi = torch.cat(frames)\n",
    "    \n",
    "    return phi\n",
    "\n",
    "def get_rand_phis(k, n):\n",
    "    frames = []\n",
    "    env = gym.envs.make('Pong-v4')\n",
    "    env.reset()\n",
    "    \n",
    "    for i in range(n):\n",
    "        a = random.randrange(env.action_space.n)\n",
    "        s_t1, r_t, done, _ = env.step(a)\n",
    "        frames.append(s_t1)\n",
    "\n",
    "        if done:\n",
    "            s_t = env.reset()\n",
    "    \n",
    "    idxs = random.sample(range(3, n), k)\n",
    "    phis = [frames_to_phi(frames[i-3:i+1]) for i in idxs]\n",
    "    \n",
    "    return phis\n",
    "\n",
    "def get_frames_avg_qval(phis, agt):\n",
    "    x = torch.stack(phis)\n",
    "    qvals = agt.get_best_values(x)\n",
    "    \n",
    "    return torch.mean(qvals).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(replay_mem_size, batch_size):\n",
    "    agt = agent.DQNAgent()\n",
    "    replay_mem = utils.ReplayMemory(replay_mem_size, batch_size)\n",
    "    obs_history = utils.ObsHistory()\n",
    "    env = gym.envs.make('Pong-v4')\n",
    "    train_stats = TrainingStats()\n",
    "    \n",
    "    return agt, replay_mem, obs_history, env, train_stats\n",
    "\n",
    "def act_step(obs_history, agt, env):\n",
    "    phi_t = obs_history.get_phi()\n",
    "    a_t = agt.act(phi_t)\n",
    "    s_t1, r_t, done, _ = env.step(a_t)\n",
    "    \n",
    "    return a_t, s_t1, r_t, done\n",
    "\n",
    "def store_step(s_t, a_t, r_t, done, s_t1, obs_history, replay_mem):\n",
    "    obs_history.store(s_t1)\n",
    "    replay_mem.store(s_t, a_t, r_t, done)\n",
    "\n",
    "def gradient_step(replay_mem, agt):\n",
    "    if replay_mem.size() > replay_mem.sample_size + 3:\n",
    "        mini_batch = replay_mem.sample()\n",
    "\n",
    "        agt.optimizer.zero_grad()\n",
    "        loss = train.mini_batch_loss(mini_batch, agt)\n",
    "        loss.backward()\n",
    "        agt.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "def save_params(agt, episodes, save_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': agt.qnet.state_dict(),\n",
    "        'optimizer_state_dict': agt.optimizer.state_dict(),\n",
    "        'episodes': episodes\n",
    "    }, save_path)\n",
    "\n",
    "def load_params(agt, load_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    agt.qnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agt.opimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return chekpoint['episodes']\n",
    "\n",
    "def reset_episode(env, obs_history):\n",
    "    s_t = env.reset()\n",
    "    obs_history.reset(s_t)\n",
    "    done = False\n",
    "    \n",
    "    return s_t, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_phi(frames):\n",
    "    frames = [utils.process_frame(s) for s in frames]\n",
    "    phi = torch.cat(frames)\n",
    "\n",
    "    return phi\n",
    "\n",
    "def get_rand_phis(k, n):\n",
    "    frames = []\n",
    "    env = gym.envs.make('Pong-v4')\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(n):\n",
    "        a = random.randrange(env.action_space.n)\n",
    "        s_t1, r_t, done, _ = env.step(a)\n",
    "        frames.append(s_t1)\n",
    "\n",
    "        if done:\n",
    "            s_t = env.reset()\n",
    "\n",
    "    idxs = random.sample(range(3, n), k)\n",
    "    phis = [frames_to_phi(frames[i-3:i+1]) for i in idxs]\n",
    "\n",
    "    return phis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingStats:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ep_rewards = []\n",
    "        self.ep_avg_train_losses = []\n",
    "        self.steps_per_ep = []\n",
    "        self.benchmark_qvals = []\n",
    "        self.benchmark_frames = torch.stack(get_rand_phis(10, 10000))\n",
    "        \n",
    "    def store(self, agt, ep_reward, ep_steps, ep_loss, episode_num):\n",
    "        self.ep_rewards.append(ep_reward)\n",
    "        self.steps_per_ep.append(ep_steps)\n",
    "        avg_ep_loss = ep_loss / ep_steps\n",
    "        self.ep_avg_train_losses.append(avg_ep_loss)\n",
    "        avg_qvals = get_frames_avg_qvals(agt)\n",
    "        \n",
    "        print('Episode {}:'.format(episode_num))\n",
    "        print('Reward: {}'.format(ep_reward))\n",
    "        print('Steps: {}'.format(ep_steps))\n",
    "        print('Avg loss: {:.5f}'.format(avg_ep_loss))\n",
    "        print('===========================================')\n",
    "        \n",
    "\n",
    "    def get_frames_avg_qval(self, agt):\n",
    "        qvals = agt.get_best_values(self.benchmark_frames)\n",
    "\n",
    "        return torch.mean(qvals).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards = []\n",
    "ep_avg_train_losses = []\n",
    "steps_per_ep = []\n",
    "benchmark_qvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agt, replay_mem, obs_history, env, train_stats = \\\n",
    "    initialize(replay_mem_size, batch_size)\n",
    "\n",
    "for episode in range(num_episodes):  # loop over episodes\n",
    "    s_t, done = reset_episode(env, obs_history)\n",
    "    \n",
    "    ep_reward = 0\n",
    "    ep_train_loss = 0\n",
    "    ep_steps = 0\n",
    "    \n",
    "    while not done:  # loop over steps in episode\n",
    "        a_t, s_t1, r_t, done = act_step(obs_history, agt, env)\n",
    "        store_step(s_t, a_t, r_t, done, s_t1, obs_history, replay_mem)\n",
    "        \n",
    "        s_t = s_t1\n",
    "        \n",
    "        loss_val = gradient_step(replay_mem, agt)\n",
    "        \n",
    "        ep_reward += r_t\n",
    "        ep_steps += 1\n",
    "        if loss_val is not None:\n",
    "            ep_train_loss += loss_val \n",
    "    \n",
    "    train_stats.store(agt, ep_reward, ep_steps, ep_train_loss, episode)\n",
    "    \n",
    "    if episode % 10 == 9:\n",
    "        checkpoint_name = 'dqn_agt_{}.pt'.format(episode)\n",
    "        save_params(agt, episode, checkpoint_name)\n",
    "        print('Model saved.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Go Through One Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "from src import agent, dqn, train, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs = env.reset()\n",
    "\n",
    "for i in range(20000):\n",
    "    a = random.randrange(env.action_space.n)\n",
    "    _, _, done, _ = env.step(a)\n",
    "    time.sleep(.01)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs = env.reset()\n",
    "init_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s1, r, done, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize DQN objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_mem_size = int(1e6)\n",
    "mini_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt = agent.DQNAgent()\n",
    "replay_memory = utils.ReplayMemory(replay_mem_size, mini_batch_size)\n",
    "obs_history = utils.ObsHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin new episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_init = env.reset()  # reset environment to start new episode\n",
    "obs_history.reset(obs_init)  # reset observations for new episode\n",
    "done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = obs_history.phi\n",
    "a = agt.act(phi)\n",
    "obs, rew, done, _ = env.step(a)\n",
    "obs_history.store(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_1 = obs_history.phi\n",
    "replay_memory.store((phi, a, rew, phi_1, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Step\n",
    "\n",
    "[x] Dummy transitions function for testing.\n",
    "\n",
    "[x] Make `r` reward vector from transitions.\n",
    "\n",
    "[x] Make $\\max_{a}Q(s', a')$ vector from transitions.\n",
    "\n",
    "[x] Make `y` target vector from transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "                        ['phi', 'a', 'r', 'phi_1', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_transitions(n):\n",
    "    transitions = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        phi = torch.empty(4, 84, 84).random_(0, 255)\n",
    "        phi_1 = torch.empty(4, 84, 84).random_(0, 255)\n",
    "        a = np.random.randint(0, 6)\n",
    "        r = np.random.randint(0, 2)\n",
    "        done = False if np.random.randint(0, 2) == 0 else True\n",
    "        \n",
    "        transitions.append(Transition(phi, a, r, phi_1, done))\n",
    "    \n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = dummy_transitions(3)\n",
    "    \n",
    "phi, a, r, phi_1, done = zip(*transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y(transitions, agt):\n",
    "    y = []\n",
    "    \n",
    "    for tr in transitions:\n",
    "        if tr.done:\n",
    "            y.append(tr.r)\n",
    "        else:\n",
    "            x = tr.phi.unsqueeze(0)\n",
    "            y.append(tr.r + .99 * agt.get_best_values(x).item())\n",
    "            \n",
    "    return torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = make_y(transitions, agt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_vals(transitions, agt):\n",
    "    phis = []\n",
    "\n",
    "    for tr in transitions:\n",
    "        phis.append(tr.phi)\n",
    "\n",
    "    x = torch.stack(phis)\n",
    "    return agt.get_best_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmax = get_max_vals(transitions, agt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y, qmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_loss(transitions, agt):\n",
    "    y = make_y(transitions, agt)\n",
    "    qmax = get_max_vals(transitions, agt)\n",
    "    \n",
    "    loss = nn.MSELoss(reduction='mean')\n",
    "    return loss(y, qmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss = mini_batch_loss(transitions, agt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.RMSprop(agt.qnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss = mini_batch_loss(transitions, agt)\n",
    "mb_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random agent on Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obss = []\n",
    "obss.append(env.reset())\n",
    "\n",
    "for _ in range(1000):\n",
    "    a = np.random.choice(env.action_space.n)\n",
    "    obs, rew, done, _ = env.step(a)\n",
    "    obss.append(obs)\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
